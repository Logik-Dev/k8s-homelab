apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-mistral
spec:
  template:
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          env:
            - name: OLLAMA_MODELS
              value: /app/models
          command: ["/bin/sh", "-c"]
          args:
            - |
              ollama serve &
              sleep 10
              ollama pull mistral
              wait
          resources:
            limits:
              nvidia.com/gpu-all: 1
          volumeMounts:
            - name: models
              mountPath: /app/models
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ollama-models
